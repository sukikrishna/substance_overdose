{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4332b2cf-d8d2-49ff-a631-4ad195c34633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_166496/2975654231.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Month'] = pd.to_datetime(df['Month'])\n",
      "/home/sukikrishna/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing look-back period: 3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ff1f45a9240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [8, 11]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m lstm_rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(lstm_mse)\n\u001b[1;32m     95\u001b[0m lstm_mape \u001b[38;5;241m=\u001b[39m mean_absolute_percentage_error(testY, testPredict)\n\u001b[0;32m---> 97\u001b[0m sarima_mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msarima_predictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m sarima_rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(sarima_mse)\n\u001b[1;32m     99\u001b[0m sarima_mape \u001b[38;5;241m=\u001b[39m mean_absolute_percentage_error(testY, sarima_predictions[\u001b[38;5;28mlen\u001b[39m(train):\u001b[38;5;28mlen\u001b[39m(train)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(test)])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[1;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[1;32m    504\u001b[0m         )\n\u001b[0;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:111\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m--> 111\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [8, 11]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import os\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_excel('data/state_month_overdose.xlsx')\n",
    "df['Deaths'] = df['Deaths'].apply(lambda x: 0 if x == 'Suppressed' else int(x))\n",
    "df['Month'] = pd.to_datetime(df['Month'])\n",
    "df.set_index('Month', inplace=True)\n",
    "df = df.groupby(['Month']).agg({'Deaths': 'sum'}).reset_index()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('tables', exist_ok=True)\n",
    "\n",
    "# Define functions\n",
    "def create_dataset(dataset, look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset.iloc[i:(i + look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset.iloc[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def generate_forecast(model, initial_sequence, num_predictions, look_back):\n",
    "    predictions = []\n",
    "    for _ in range(num_predictions):\n",
    "        next_prediction = model.predict(initial_sequence)\n",
    "        predictions.append(next_prediction[0][0])\n",
    "        new_sequence = np.append(initial_sequence[0, 1:], [[next_prediction[0][0]]], axis=0)\n",
    "        initial_sequence = new_sequence.reshape((1, look_back, 1))\n",
    "    return np.array(predictions)\n",
    "\n",
    "def calculate_confidence_intervals(predictions):\n",
    "    mean_pred = np.mean(predictions)\n",
    "    std_pred = np.std(predictions)\n",
    "    z_score = 1.96  # for 95% confidence\n",
    "    margin_of_error = z_score * (std_pred / np.sqrt(len(predictions)))\n",
    "    lower_bound = predictions - margin_of_error\n",
    "    upper_bound = predictions + margin_of_error\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def calculate_overlap(lower1, upper1, lower2, upper2):\n",
    "    overlap_count = 0\n",
    "    for l1, u1, l2, u2 in zip(lower1, upper1, lower2, upper2):\n",
    "        if u1 >= l2 and l1 <= u2:\n",
    "            overlap_count += 1\n",
    "    percent_overlap = (overlap_count / len(lower1)) * 100\n",
    "    return percent_overlap\n",
    "\n",
    "# Iterate over look-back periods\n",
    "look_back_periods = range(3, 12, 2)\n",
    "results = []\n",
    "\n",
    "for look_back in look_back_periods:\n",
    "    print(f\"Processing look-back period: {look_back}\")\n",
    "\n",
    "    # Split data\n",
    "    train = df[df['Month'] <= '2020-02-01']\n",
    "    test = df[df['Month'] >= '2020-02-01']\n",
    "\n",
    "    # Create datasets\n",
    "    trainX, trainY = create_dataset(train['Deaths'], look_back)\n",
    "    testX, testY = create_dataset(test['Deaths'], look_back)\n",
    "\n",
    "    # Reshape for LSTM\n",
    "    trainX = trainX.reshape((trainX.shape[0], look_back, 1))\n",
    "    testX = testX.reshape((testX.shape[0], look_back, 1))\n",
    "\n",
    "    # Build LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(look_back, 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY, epochs=50, batch_size=1, verbose=0)\n",
    "\n",
    "    # Generate forecasts\n",
    "    initial_sequence = trainX[-1].reshape((1, look_back, 1))\n",
    "    testPredict = generate_forecast(model, initial_sequence, len(testY), look_back)\n",
    "    trainPredict = model.predict(trainX).flatten()\n",
    "\n",
    "    # SARIMA model\n",
    "    sarima_model = SARIMAX(train['Deaths'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12),\n",
    "                           enforce_stationarity=False, enforce_invertibility=False)\n",
    "    sarima_result = sarima_model.fit(disp=False)\n",
    "    sarima_predictions = sarima_result.predict(start=0, end=len(train) + len(test) - 1, dynamic=False)\n",
    "\n",
    "    # Error metrics\n",
    "    lstm_mse = mean_squared_error(testY, testPredict)\n",
    "    lstm_rmse = np.sqrt(lstm_mse)\n",
    "    lstm_mape = mean_absolute_percentage_error(testY, testPredict)\n",
    "\n",
    "    sarima_mse = mean_squared_error(testY, sarima_predictions[len(train):len(train)+len(test)])\n",
    "    sarima_rmse = np.sqrt(sarima_mse)\n",
    "    sarima_mape = mean_absolute_percentage_error(testY, sarima_predictions[len(train):len(train)+len(test)])\n",
    "\n",
    "    # Confidence intervals and overlap\n",
    "    lower_bound_test, upper_bound_test = calculate_confidence_intervals(testPredict)\n",
    "    sarimaTestPredict = sarima_predictions[len(train):len(train)+len(test)]\n",
    "    lower_bound_sarima, upper_bound_sarima = calculate_confidence_intervals(sarimaTestPredict)\n",
    "    ci_overlap = calculate_overlap(lower_bound_test, upper_bound_test, lower_bound_sarima, upper_bound_sarima)\n",
    "\n",
    "    # Save results\n",
    "    results.append({\n",
    "        'Look-back': look_back,\n",
    "        'LSTM MAPE': lstm_mape,\n",
    "        'LSTM MSE': lstm_mse,\n",
    "        'LSTM RMSE': lstm_rmse,\n",
    "        'SARIMA MAPE': sarima_mape,\n",
    "        'SARIMA MSE': sarima_mse,\n",
    "        'SARIMA RMSE': sarima_rmse,\n",
    "        'CI Overlap %': ci_overlap\n",
    "    })\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    df[f'LSTM Predictions ({look_back})'] = [0] * look_back + list(trainPredict) + list(testPredict)\n",
    "    df[f'SARIMA Predictions ({look_back})'] = sarima_predictions\n",
    "    df.to_csv(f'tables/{look_back}_month_predictionresults.csv', index=False)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['Month'], df['Deaths'], label='Actual Data', color='blue')\n",
    "    plt.plot(df['Month'], df[f'LSTM Predictions ({look_back})'], label=f'LSTM Predictions ({look_back} months)', color='red')\n",
    "    plt.plot(df['Month'], df[f'SARIMA Predictions ({look_back})'], label=f'SARIMA Predictions ({look_back} months)', color='green')\n",
    "    plt.title(f'Deaths: Actual vs Predictions (Look-back: {look_back})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Deaths')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'tables/{look_back}_month_prediction_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "# Save summary results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('tables/summary_metrics.csv', index=False)\n",
    "\n",
    "print(\"Processing complete. Results saved to 'tables' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f64cfa-744f-42c5-a86b-9916ebe04d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
