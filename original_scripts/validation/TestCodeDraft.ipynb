{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56a70b78-c4d9-4144-a61f-3bdffb24b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 6\n",
    "batch_size = 1\n",
    "loss = 'mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c3a60f-8993-4860-b15c-862e616f74f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 00:08:10.753343: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-19 00:08:10.776247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-19 00:08:10.816384: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-19 00:08:10.823190: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-19 00:08:10.838300: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-19 00:08:11.908672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/tmp/ipykernel_14570/1068810766.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Month'] = pd.to_datetime(df['Month'])\n",
      "/tmp/ipykernel_14570/1068810766.py:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Month Code'] = pd.to_datetime(df['Month Code'])#.reset_index() #2015-01-01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.read_excel('../data/state_month_overdose.xlsx')\n",
    "\n",
    "df['Deaths'] = df['Deaths'].apply(lambda x: 0 if x == 'Suppressed' else int(x))\n",
    "\n",
    "df['Month'] = pd.to_datetime(df['Month'])\n",
    "df.set_index('Month', inplace=True)\n",
    "\n",
    "df = df.reset_index() #2015/01\n",
    "df['Month Code'] = pd.to_datetime(df['Month Code'])#.reset_index() #2015-01-01\n",
    "# df.set_index('Month', inplace=True)\n",
    "df = df.groupby(['Month']).agg({'Deaths': 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "train = df[df['Month'] < '2019-01-01']\n",
    "test = df[(df['Month'] >= '2019-01-01') & (df['Month'] <= '2019-12-01')]\n",
    "testog = test\n",
    "test = test.reset_index().drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cde3791-8dd5-47db-b92a-d90381cdb7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>3931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>3727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>4285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>3934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>4100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>4112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>4371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>4252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>4529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>4560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>4645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Month  Deaths\n",
       "0  2019-01-01    3931\n",
       "1  2019-02-01    3727\n",
       "2  2019-03-01    4285\n",
       "3  2019-04-01    3934\n",
       "4  2019-05-01    4100\n",
       "5  2019-06-01    4112\n",
       "6  2019-07-01    4296\n",
       "7  2019-08-01    4371\n",
       "8  2019-09-01    4252\n",
       "9  2019-10-01    4529\n",
       "10 2019-11-01    4560\n",
       "11 2019-12-01    4645"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec92072f-b1f3-4e15-a8f0-affa40724ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the create_dataset function to use a lookback of 3 months\n",
    "def create_dataset(dataset, look_back=3):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset.iloc[i:(i + look_back)]  # Collect the previous 'look_back' months\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset.iloc[i + look_back])  # The target is the subsequent month\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "look_back = 3\n",
    "\n",
    "extended_test = pd.concat([train.iloc[-look_back:], test])\n",
    "\n",
    "# Prepare LSTM datasets\n",
    "trainX, trainY = create_dataset(train['Deaths'], look_back)\n",
    "testX, testY = create_dataset(extended_test['Deaths'], look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947f6775-de3a-43dd-99e7-c309591b5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape inputs to match LSTM input requirements (samples, time_steps, features)\n",
    "trainX = trainX.reshape((trainX.shape[0], look_back, 1))\n",
    "testX = testX.reshape((testX.shape[0], look_back, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cce7528-64f1-490b-8222-111e8da8538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737274093.260769   14570 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-19 00:08:13.349551: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/sukikrishna/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 15870202.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9216045.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 2795126.2500\n",
      "Epoch 4/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - loss: 67137.8359\n",
      "Epoch 5/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - loss: 29304.8359\n",
      "Epoch 6/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - loss: 33428.1641\n",
      "Epoch 7/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 64718.8281  \n",
      "Epoch 8/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 40285.0742 \n",
      "Epoch 9/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 63550.0039\n",
      "Epoch 10/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 46482.4492\n",
      "Epoch 11/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 44858.6602 \n",
      "Epoch 12/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - loss: 49832.2578\n",
      "Epoch 13/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 46472.9844\n",
      "Epoch 14/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 48777.7227\n",
      "Epoch 15/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 38792.7500 \n",
      "Epoch 16/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - loss: 52944.4375\n",
      "Epoch 17/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - loss: 42108.5547\n",
      "Epoch 18/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 56629.3711\n",
      "Epoch 19/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 63729.6875\n",
      "Epoch 20/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - loss: 35274.2812\n",
      "Epoch 21/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 40605.6797\n",
      "Epoch 22/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 48037.2188\n",
      "Epoch 23/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 37355.9805\n",
      "Epoch 24/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 46850.1953\n",
      "Epoch 25/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 60743.7969\n",
      "Epoch 26/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 45270.7539\n",
      "Epoch 27/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 56097.8555\n",
      "Epoch 28/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step - loss: 48328.4492\n",
      "Epoch 29/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - loss: 55363.7539\n",
      "Epoch 30/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 48917.6484\n",
      "Epoch 31/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 45158.7031\n",
      "Epoch 32/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 35846.8633\n",
      "Epoch 33/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - loss: 37854.1602\n",
      "Epoch 34/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step - loss: 54020.7578\n",
      "Epoch 35/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973us/step - loss: 55775.0938\n",
      "Epoch 36/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58420.9727 \n",
      "Epoch 37/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 44296.6914\n",
      "Epoch 38/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 38184.1797\n",
      "Epoch 39/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 54450.1484\n",
      "Epoch 40/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 56164.7383\n",
      "Epoch 41/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 68796.3594\n",
      "Epoch 42/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53618.9375\n",
      "Epoch 43/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51756.5625\n",
      "Epoch 44/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 33307.6992\n",
      "Epoch 45/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 44101.7930\n",
      "Epoch 46/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 41516.9102\n",
      "Epoch 47/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 34511.2070\n",
      "Epoch 48/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - loss: 53044.7852\n",
      "Epoch 49/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 56818.4531\n",
      "Epoch 50/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 51835.7383\n",
      "Epoch 51/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - loss: 52753.2695\n",
      "Epoch 52/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 34667.9141\n",
      "Epoch 53/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 71498.8984\n",
      "Epoch 54/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 54688.4102\n",
      "Epoch 55/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - loss: 40657.0156\n",
      "Epoch 56/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 26647.2559\n",
      "Epoch 57/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 64255.3828\n",
      "Epoch 58/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 40718.9180\n",
      "Epoch 59/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 39822.4375\n",
      "Epoch 60/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 49014.7969\n",
      "Epoch 61/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 50032.8594\n",
      "Epoch 62/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 53546.3555\n",
      "Epoch 63/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 64672.8398 \n",
      "Epoch 64/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 57530.7188\n",
      "Epoch 65/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 57613.6992\n",
      "Epoch 66/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 38452.1367\n",
      "Epoch 67/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 45762.1367\n",
      "Epoch 68/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 75717.9922\n",
      "Epoch 69/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 65868.6328\n",
      "Epoch 70/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 44635.3086\n",
      "Epoch 71/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 56089.9922\n",
      "Epoch 72/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 45662.5703\n",
      "Epoch 73/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 63509.4805\n",
      "Epoch 74/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 53854.3047\n",
      "Epoch 75/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 46238.0195\n",
      "Epoch 76/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 44972.9453\n",
      "Epoch 77/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 44498.7500\n",
      "Epoch 78/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 40147.4688\n",
      "Epoch 79/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 64985.0508\n",
      "Epoch 80/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 46785.9219\n",
      "Epoch 81/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 64850.4922\n",
      "Epoch 82/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 52436.4766\n",
      "Epoch 83/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 34609.3828\n",
      "Epoch 84/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 55962.3125\n",
      "Epoch 85/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 38447.2109\n",
      "Epoch 86/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 54213.1133\n",
      "Epoch 87/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 57945.7344\n",
      "Epoch 88/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 49240.2422\n",
      "Epoch 89/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step - loss: 29763.2637\n",
      "Epoch 90/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 32576.8594\n",
      "Epoch 91/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 60254.7734\n",
      "Epoch 92/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 43189.6953\n",
      "Epoch 93/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 52195.6836\n",
      "Epoch 94/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 39546.7422\n",
      "Epoch 95/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 42379.9609\n",
      "Epoch 96/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 59212.9727\n",
      "Epoch 97/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71656.7734 \n",
      "Epoch 98/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 40580.9531\n",
      "Epoch 99/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59377.7852\n",
      "Epoch 100/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 40105.8438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f3ff40f5870>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rebuild the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(look_back, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=1)\n",
    "\n",
    "# model = Sequential([\n",
    "#     LSTM(50, return_sequences=False, input_shape=(look_back, 1)),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852e55c4-9352-4fdd-832f-532d418e859f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Running\n"
     ]
    }
   ],
   "source": [
    "print('Finished Running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1e5908-078c-4e69-a638-1a938c58244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated generate_forecast function to handle 3-month lookback\n",
    "def generate_forecast(model, initial_sequence, num_predictions=12, look_back=3):\n",
    "    predictions = []\n",
    "    for _ in range(num_predictions):\n",
    "        # Generate the next prediction\n",
    "        next_prediction = model.predict(initial_sequence)\n",
    "        predictions.append(next_prediction[0][0])\n",
    "        \n",
    "        # Update the sequence with the new prediction\n",
    "        new_sequence = np.append(initial_sequence[0, 1:], [[next_prediction[0][0]]], axis=0)\n",
    "        initial_sequence = new_sequence.reshape((1, look_back, 1))\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca06f978-dbaf-447d-9064-923861fb823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3882],\n",
       "        [3999],\n",
       "        [3692]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the initial sequence for forecasting using the last `look_back` months from training\n",
    "initial_sequence = trainX[-1].reshape((1, look_back, 1))\n",
    "initial_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b95c46c6-8d40-48fd-806f-fd21dc0bc780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n"
     ]
    }
   ],
   "source": [
    "# Generate test predictions with the updated lookback logic\n",
    "testPredict = generate_forecast(model, initial_sequence, num_predictions=testY.shape[0], look_back=look_back)\n",
    "trainPredict = model.predict(trainX)\n",
    "\n",
    "# Flatten predictions for visualization and evaluation\n",
    "testPredictlst = testPredict.flatten().tolist()\n",
    "trainPredictlst = trainPredict.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68c366d-3446-4a38-a1fe-d882ba635588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testPredictlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f874a16d-0639-411a-b3da-e34162dd8929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainPredictlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19920d6a-cb3a-4e14-b3b5-4fda75fa12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array = [0] * look_back + trainPredictlst + testPredictlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed3bcab5-629f-40f3-889e-8999b2e62e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>2771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>2627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>2907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>2850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>2634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>2734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-08-01</td>\n",
       "      <td>2810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-09-01</td>\n",
       "      <td>2925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>2739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>2718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>3416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>3721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>3608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>3475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>3384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>3668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>3624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>3518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>3695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>3745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>4044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>4006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>4252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>4056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>4129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>3950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>3980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>4022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>3832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>3820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>3671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>4155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>3922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>4079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>4063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>4085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>4061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>3882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>3999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>3692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>3976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>3931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>3727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>4285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>3934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>4100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>4112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>4371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>4252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>4529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>4560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>4645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Month  Deaths\n",
       "0  2015-01-01    2771\n",
       "1  2015-02-01    2627\n",
       "2  2015-03-01    2907\n",
       "3  2015-04-01    2822\n",
       "4  2015-05-01    2850\n",
       "5  2015-06-01    2634\n",
       "6  2015-07-01    2734\n",
       "7  2015-08-01    2810\n",
       "8  2015-09-01    2925\n",
       "9  2015-10-01    2970\n",
       "10 2015-11-01    2739\n",
       "11 2015-12-01    2718\n",
       "12 2016-01-01    3098\n",
       "13 2016-02-01    3416\n",
       "14 2016-03-01    3721\n",
       "15 2016-04-01    3608\n",
       "16 2016-05-01    3475\n",
       "17 2016-06-01    3384\n",
       "18 2016-07-01    3668\n",
       "19 2016-08-01    3624\n",
       "20 2016-09-01    3518\n",
       "21 2016-10-01    3695\n",
       "22 2016-11-01    3745\n",
       "23 2016-12-01    4044\n",
       "24 2017-01-01    4332\n",
       "25 2017-02-01    4006\n",
       "26 2017-03-01    4252\n",
       "27 2017-04-01    4056\n",
       "28 2017-05-01    4129\n",
       "29 2017-06-01    3950\n",
       "30 2017-07-01    3980\n",
       "31 2017-08-01    4000\n",
       "32 2017-09-01    4022\n",
       "33 2017-10-01    3832\n",
       "34 2017-11-01    3820\n",
       "35 2017-12-01    4017\n",
       "36 2018-01-01    4008\n",
       "37 2018-02-01    3671\n",
       "38 2018-03-01    4155\n",
       "39 2018-04-01    3922\n",
       "40 2018-05-01    4079\n",
       "41 2018-06-01    4063\n",
       "42 2018-07-01    4085\n",
       "43 2018-08-01    4061\n",
       "44 2018-09-01    3882\n",
       "45 2018-10-01    3999\n",
       "46 2018-11-01    3692\n",
       "47 2018-12-01    3976\n",
       "48 2019-01-01    3931\n",
       "49 2019-02-01    3727\n",
       "50 2019-03-01    4285\n",
       "51 2019-04-01    3934\n",
       "52 2019-05-01    4100\n",
       "53 2019-06-01    4112\n",
       "54 2019-07-01    4296\n",
       "55 2019-08-01    4371\n",
       "56 2019-09-01    4252\n",
       "57 2019-10-01    4529\n",
       "58 2019-11-01    4560\n",
       "59 2019-12-01    4645"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = df[df['Month'] <= '2019-12-01']\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a0b4ef4-1b05-4213-8e7b-474afa5880db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f048ed7b-7063-4a4b-87ab-669182ee6aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0e2ce5-a100-4fbf-a22d-03e28deecf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3999],\n",
       "        [3692],\n",
       "        [3976]],\n",
       "\n",
       "       [[3692],\n",
       "        [3976],\n",
       "        [3931]],\n",
       "\n",
       "       [[3976],\n",
       "        [3931],\n",
       "        [3727]],\n",
       "\n",
       "       [[3931],\n",
       "        [3727],\n",
       "        [4285]],\n",
       "\n",
       "       [[3727],\n",
       "        [4285],\n",
       "        [3934]],\n",
       "\n",
       "       [[4285],\n",
       "        [3934],\n",
       "        [4100]],\n",
       "\n",
       "       [[3934],\n",
       "        [4100],\n",
       "        [4112]],\n",
       "\n",
       "       [[4100],\n",
       "        [4112],\n",
       "        [4296]],\n",
       "\n",
       "       [[4112],\n",
       "        [4296],\n",
       "        [4371]],\n",
       "\n",
       "       [[4296],\n",
       "        [4371],\n",
       "        [4252]],\n",
       "\n",
       "       [[4371],\n",
       "        [4252],\n",
       "        [4529]],\n",
       "\n",
       "       [[4252],\n",
       "        [4529],\n",
       "        [4560]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "104ed23a-3416-4f6a-9741-c94aad131e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sukikrishna/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n",
      "  warn('Too few observations to estimate starting parameters%s.'\n"
     ]
    }
   ],
   "source": [
    "sarima_model = SARIMAX(train['Deaths'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False)\n",
    "sarima_result = sarima_model.fit(disp=False)\n",
    "sarima_predictions = sarima_result.predict(start=0, end=len(train) + len(test) - 1, dynamic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9b68ebb-aa31-4942-bdce-c2d5a4daa903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sarima_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7c725de-df8d-4290-9e82-7ef33f625410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14570/3914168234.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['LSTM Predictions'] = combined_array\n",
      "/tmp/ipykernel_14570/3914168234.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['SARIMA Predictions'] = sarima_predictions\n"
     ]
    }
   ],
   "source": [
    "val_df['LSTM Predictions'] = combined_array\n",
    "val_df['SARIMA Predictions'] = sarima_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d2dd845-8b5c-45b8-ba95-382cdff5ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(f'../tables/{look_back}month_predictionresults_batch_{batch_size}_loss_{loss}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a15fa-8f0f-4bf0-a0fb-e3c802243035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
