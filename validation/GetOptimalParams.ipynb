{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7049d58b-6ae6-4150-ac57-520f33a74619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa00813-080f-4244-8211-aa8ddbe59456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the folders\n",
    "base_dir = \"../tables\"\n",
    "\n",
    "# Output file\n",
    "output_file = \"hyper_csv_metrics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81456d4-7bdd-4f0f-985b-7bce54698608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate confidence intervals\n",
    "def calculate_confidence_intervals(predictions, alpha=0.05):\n",
    "    mean_pred = np.mean(predictions)\n",
    "    std_pred = np.std(predictions)\n",
    "    z_score = 1.96  # 95% confidence\n",
    "    margin_of_error = z_score * (std_pred / np.sqrt(len(predictions)))\n",
    "    lower_bound = predictions - margin_of_error\n",
    "    upper_bound = predictions + margin_of_error\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Function to calculate prediction intervals\n",
    "def calculate_prediction_intervals(actual, predictions, alpha=0.05):\n",
    "    residuals = actual - predictions\n",
    "    std_residual = np.std(residuals)\n",
    "    z_score = 1.96  # 95% confidence\n",
    "    margin_of_error = z_score * std_residual\n",
    "    lower_bound = predictions - margin_of_error\n",
    "    upper_bound = predictions + margin_of_error\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Function to calculate percent overlap\n",
    "def calculate_overlap(lower1, upper1, lower2, upper2):\n",
    "    overlap_count = sum((u1 >= l2) & (l1 <= u2) for l1, u1, l2, u2 in zip(lower1, upper1, lower2, upper2))\n",
    "    percent_overlap = (overlap_count / len(lower1)) * 100\n",
    "    return percent_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1becf3f8-4f85-4053-8373-15510a0446d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m folder_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(root)\n\u001b[1;32m      3\u001b[0m file_name \u001b[38;5;241m=\u001b[39m file\n\u001b[0;32m----> 4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m loss_type \u001b[38;5;241m=\u001b[39m folder_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      6\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(folder_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'checkpoints'"
     ]
    }
   ],
   "source": [
    "file = '3month_predictionresults_batch_8_loss_mean_squared_error_epochs_100.csv'\n",
    "folder_name = os.path.basename(root)\n",
    "file_name = file\n",
    "batch_size = int(folder_name.split(\"_\")[1])\n",
    "loss_type = folder_name.split(\"_\")[3]\n",
    "epochs = int(folder_name.split(\"_\")[-1])\n",
    "lookback = int(file_name.split(\"month\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110d5f9c-cc38-4173-a0c4-1c954781741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3month_predictionresults_batch_8_loss_mean_squared_error_epochs_100.csv\n",
      "6month_predictionresults_batch_16_loss_mean_squared_error_epochs_50.csv\n",
      "5month_predictionresults_batch_1_loss_mean_squared_error_epochs_50.csv\n",
      "5month_predictionresults_batch_8_loss_mean_squared_error_epochs_50.csv\n",
      "3month_predictionresults_batch_16_loss_mean_squared_error_epochs_100.csv\n",
      "5month_predictionresults_batch_1_loss_mean_squared_error_epochs_100.csv\n",
      "3month_predictionresults_batch_1_loss_mean_squared_error_epochs_100-checkpoint.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m folder_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(root)\n\u001b[1;32m     10\u001b[0m file_name \u001b[38;5;241m=\u001b[39m file\n\u001b[0;32m---> 11\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss_type \u001b[38;5;241m=\u001b[39m folder_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     13\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(folder_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'checkpoints'"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Traverse the directory structure\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            print(file)\n",
    "            folder_name = os.path.basename(root)\n",
    "            file_name = file\n",
    "            batch_size = int(folder_name.split(\"_\")[1])\n",
    "            loss_type = folder_name.split(\"_\")[3]\n",
    "            epochs = int(folder_name.split(\"_\")[-1])\n",
    "            lookback = int(file_name.split(\"month\")[0])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a239874d-a381-4043-ac34-7e5cb8e6203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyper_csv(input_directory, output_file):\n",
    "    # Prepare the final hyper CSV structure\n",
    "    results = []\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file.endswith('.csv'):\n",
    "            filepath = os.path.join(input_directory, file)\n",
    "            try:\n",
    "                # Parse hyperparameters from filename (adjusting to ignore 'loss')\n",
    "                parts = file.split('_')\n",
    "                lookback_period = int(parts[0].replace('month', ''))\n",
    "                batch_size = int(parts[3].replace('batch', ''))\n",
    "                epochs = int(parts[-1].replace('epochs', '').replace('.csv', ''))\n",
    "\n",
    "                # Read the CSV\n",
    "                df = pd.read_csv(filepath)\n",
    "                original = df[\"Deaths\"]\n",
    "                lstm = df[\"LSTM Predictions\"]\n",
    "                sarima = df[\"SARIMA Predictions\"]\n",
    "\n",
    "                # Calculate metrics for LSTM and SARIMA\n",
    "                lstm_rmse, lstm_mape = calculate_metrics(original, lstm)\n",
    "                sarima_rmse, sarima_mape = calculate_metrics(original, sarima)\n",
    "\n",
    "                # Confidence intervals and prediction intervals\n",
    "                lower_lstm, upper_lstm = calculate_prediction_intervals(original, lstm)\n",
    "                lower_sarima, upper_sarima = calculate_prediction_intervals(original, sarima)\n",
    "\n",
    "                # Calculate percent overlap\n",
    "                percent_overlap = calculate_overlap(lower_lstm, upper_lstm, lower_sarima, upper_sarima)\n",
    "\n",
    "                # Append results\n",
    "                results.append({\n",
    "                    \"Lookback Period\": lookback_period,\n",
    "                    \"Batch Size\": batch_size,\n",
    "                    \"Epochs\": epochs,\n",
    "                    \"LSTM RMSE\": lstm_rmse,\n",
    "                    \"LSTM MAPE\": lstm_mape,\n",
    "                    \"SARIMA RMSE\": sarima_rmse,\n",
    "                    \"SARIMA MAPE\": sarima_mape,\n",
    "                    \"Percent Overlap\": percent_overlap\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "    # Save results to a new CSV file\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Hyper CSV created at {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82cbe8c9-f818-4fb6-baf5-c9c304a58907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper CSV created at hyper_results_1_50.csv\n"
     ]
    }
   ],
   "source": [
    "input_directory = \"../tables/batch_1_loss_mse_epoch_50\"  # Replace with your directory\n",
    "output_file = \"hyper_results_1_50.csv\"\n",
    "create_hyper_csv(input_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeac2512-28e3-484a-8f0e-c6f792ff9bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSVs have been stacked into one DataFrame.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def stack_csvs(input_directory):\n",
    "    \"\"\"\n",
    "    Stacks all CSV files in a directory into one DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): Path to the directory containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame containing data from all CSV files.\n",
    "    \"\"\"\n",
    "    csv_list = []  # List to store individual DataFrames\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file.endswith('.csv'):  # Check if the file is a CSV\n",
    "            filepath = os.path.join(input_directory, file)\n",
    "            try:\n",
    "                # Read the CSV file and append to the list\n",
    "                df = pd.read_csv(filepath)\n",
    "                csv_list.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    # Combine all DataFrames in the list into one\n",
    "    combined_df = pd.concat(csv_list, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# Example usage\n",
    "input_directory = \"results\"  # Replace with the path to your directory\n",
    "combined_dataframe = stack_csvs(input_directory)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file (optional)\n",
    "combined_dataframe.to_csv(\"combined_output.csv\", index=False)\n",
    "\n",
    "print(\"All CSVs have been stacked into one DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0a580-deec-405b-978a-30b1f1bb0ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
