{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3727cbc5-c953-4e45-ba75-18f51d2e122c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m         results_dict[lookback][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_std\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(lstm_std)\n\u001b[1;32m     66\u001b[0m         results_dict[lookback][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msarima_std\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(sarima_std)\n\u001b[0;32m---> 68\u001b[0m         percent_overlap \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_overlap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlower_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_sarima\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_sarima\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         results_dict[lookback][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpercent_overlap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(percent_overlap)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Compute averages and standard deviations\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36mcalculate_overlap\u001b[0;34m(lower1, upper1, lower2, upper2)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_overlap\u001b[39m(lower1, upper1, lower2, upper2):\n\u001b[1;32m     22\u001b[0m     overlap_count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((upper1 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m lower2) \u001b[38;5;241m&\u001b[39m (lower1 \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m upper2))\n\u001b[0;32m---> 23\u001b[0m     percent_overlap \u001b[38;5;241m=\u001b[39m (overlap_count \u001b[38;5;241m/\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlower1\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m percent_overlap\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# Set the base directory\n",
    "base_dir = \"seed42/batch_1_loss_mse_epoch_100\"\n",
    "output_file = os.path.join(\"aggregated_results.csv\")\n",
    "\n",
    "# Function to compute prediction intervals\n",
    "def calculate_prediction_intervals(actual, predictions, alpha=0.05):\n",
    "    residuals = actual - predictions\n",
    "    std_residual = np.std(residuals, axis=0)\n",
    "    z_score = 1.96  # 95% confidence\n",
    "    margin_of_error = z_score * std_residual\n",
    "    lower_bound = predictions - margin_of_error\n",
    "    upper_bound = predictions + margin_of_error\n",
    "    return lower_bound.mean(), upper_bound.mean(), std_residual.mean()\n",
    "\n",
    "# Function to calculate percent overlap\n",
    "def calculate_overlap(lower1, upper1, lower2, upper2):\n",
    "    overlap_count = np.sum((upper1 >= lower2) & (lower1 <= upper2), axis=1)\n",
    "    percent_overlap = (overlap_count / lower1.shape[1]) * 100\n",
    "    return np.mean(percent_overlap)\n",
    "\n",
    "# Dictionary to store results\n",
    "results_dict = {}\n",
    "\n",
    "# Traverse the directory\n",
    "for file in os.listdir(base_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_parts = file.split(\"_\")\n",
    "        trial_num = int(file_parts[1])\n",
    "        lookback = int(file_parts[2][:-5])  # Extract lookback period\n",
    "        batch_size = int(file_parts[5])\n",
    "        epochs = int(file_parts[-1][:-4])  # Extract epochs from filename\n",
    "        \n",
    "        # Load CSV\n",
    "        file_path = os.path.join(base_dir, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        original_test = df[\"Deaths\"].iloc[48:].values\n",
    "        lstm_test = df[\"LSTM Predictions\"].iloc[48:].values\n",
    "        sarima_test = df[\"SARIMA Predictions\"].iloc[48:].values\n",
    "        \n",
    "        # Store trial data\n",
    "        if lookback not in results_dict:\n",
    "            results_dict[lookback] = {\n",
    "                \"batch_size\": batch_size, \"epochs\": epochs,\n",
    "                \"lstm_rmse\": [], \"lstm_mape\": [], \"sarima_rmse\": [], \"sarima_mape\": [],\n",
    "                \"lstm_lower\": [], \"lstm_upper\": [], \"sarima_lower\": [], \"sarima_upper\": [],\n",
    "                \"lstm_std\": [], \"sarima_std\": [], \"percent_overlap\": []\n",
    "            }\n",
    "        \n",
    "        results_dict[lookback][\"lstm_rmse\"].append(np.sqrt(mean_squared_error(original_test, lstm_test)))\n",
    "        results_dict[lookback][\"lstm_mape\"].append(mean_absolute_percentage_error(original_test, lstm_test) * 100)\n",
    "        results_dict[lookback][\"sarima_rmse\"].append(np.sqrt(mean_squared_error(original_test, sarima_test)))\n",
    "        results_dict[lookback][\"sarima_mape\"].append(mean_absolute_percentage_error(original_test, sarima_test) * 100)\n",
    "        \n",
    "        lower_lstm, upper_lstm, lstm_std = calculate_prediction_intervals(original_test, lstm_test)\n",
    "        lower_sarima, upper_sarima, sarima_std = calculate_prediction_intervals(original_test, sarima_test)\n",
    "        results_dict[lookback][\"lstm_lower\"].append(lower_lstm)\n",
    "        results_dict[lookback][\"lstm_upper\"].append(upper_lstm)\n",
    "        results_dict[lookback][\"sarima_lower\"].append(lower_sarima)\n",
    "        results_dict[lookback][\"sarima_upper\"].append(upper_sarima)\n",
    "        results_dict[lookback][\"lstm_std\"].append(lstm_std)\n",
    "        results_dict[lookback][\"sarima_std\"].append(sarima_std)\n",
    "        \n",
    "        percent_overlap = calculate_overlap(lower_lstm, upper_lstm, lower_sarima, upper_sarima)\n",
    "        results_dict[lookback][\"percent_overlap\"].append(percent_overlap)\n",
    "\n",
    "# Compute averages and standard deviations\n",
    "final_results = []\n",
    "for lookback, data in sorted(results_dict.items()):\n",
    "    final_results.append({\n",
    "        \"Lookback Period\": lookback,\n",
    "        \"Batch Size\": data[\"batch_size\"],\n",
    "        \"Epochs\": data[\"epochs\"],\n",
    "        \"LSTM RMSE\": np.mean(data[\"lstm_rmse\"]),\n",
    "        \"LSTM RMSE Std\": np.std(data[\"lstm_rmse\"]),\n",
    "        \"LSTM MAPE\": np.mean(data[\"lstm_mape\"]),\n",
    "        \"LSTM MAPE Std\": np.std(data[\"lstm_mape\"]),\n",
    "        \"SARIMA RMSE\": np.mean(data[\"sarima_rmse\"]),\n",
    "        \"SARIMA RMSE Std\": np.std(data[\"sarima_rmse\"]),\n",
    "        \"SARIMA MAPE\": np.mean(data[\"sarima_mape\"]),\n",
    "        \"SARIMA MAPE Std\": np.std(data[\"sarima_mape\"]),\n",
    "        \"LSTM Prediction Interval Lower\": np.mean(data[\"lstm_lower\"]),\n",
    "        \"LSTM Prediction Interval Upper\": np.mean(data[\"lstm_upper\"]),\n",
    "        \"SARIMA Prediction Interval Lower\": np.mean(data[\"sarima_lower\"]),\n",
    "        \"SARIMA Prediction Interval Upper\": np.mean(data[\"sarima_upper\"]),\n",
    "        \"LSTM Std\": np.mean(data[\"lstm_std\"]),\n",
    "        \"SARIMA Std\": np.mean(data[\"sarima_std\"]),\n",
    "        \"Percent Overlap\": np.mean(data[\"percent_overlap\"])\n",
    "    })\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(final_results)\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"Aggregated results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b99de-7712-415d-b694-2987de4fa9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
